{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheep Counting Using Object Detection and ByteTrack\n",
    "## **Line zone**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "VIDEO_PATH = \"../../data/Videos\"\n",
    "print(VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"sheep_crossing.MP4\"\n",
    "SOURCE_VIDEO_PATH = f\"{VIDEO_PATH}/{FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SOURCE_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "print(video_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define export folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT = 'path/to/export/folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model to use\n",
    "SHEEP_MODEL = \"../../Models/02_sheep_detection_v1/sheep_v1.pt\"\n",
    "print(SHEEP_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YOLO model \n",
    "model = YOLO(SHEEP_MODEL)\n",
    "model.fuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Video Processing and Object Detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection of objects on the video and recording of a new video with object predictions according to the YOLO model used\n",
    "%cd {HOME}\n",
    "!yolo task=detect mode=predict model=\"path/to/detector.pt\" conf=0.50 source={SOURCE_VIDEO_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrating ByteTrack for Object Tracking and Sheep Counting Using a Virtual Line:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a video generator for our sample input file and display its first frame on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = next(generator)\n",
    "\n",
    "sv.plot_image(frame, (12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the position of the virtual line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line drawing\n",
    "START = sv.Point(0, 850)\n",
    "END = sv.Point(1440, 850)\n",
    "\n",
    "# define line zone\n",
    "line_zone = sv.LineZone(start=START, end=END)\n",
    "\n",
    "line_zone_annotator = sv.LineZoneAnnotator(\n",
    "    thickness=4,\n",
    "    text_thickness=4,\n",
    "    text_scale=2,\n",
    "    custom_in_text = \"in\",\n",
    "    custom_out_text = \"out\"\n",
    "    )\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = line_zone_annotator.annotate(annotated_frame, line_counter=line_zone)\n",
    "\n",
    "# show line annotator on one frame\n",
    "sv.plot_image(annotated_frame, (12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define tracking and detection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = sv.ByteTrack(\n",
    "    track_buffer=5,\n",
    "    track_thresh=0.6, # default = 0.6\n",
    "    match_thresh=0.8, # default = 0.8\n",
    "    frame_rate=video_info.fps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = sv.ColorPalette.DEFAULT\n",
    "color_type = sv.ColorLookup.TRACK\n",
    "\n",
    "# add annotators\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4, color = color, color_lookup=color_type)\n",
    "label_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=1)\n",
    "trace_annotator = sv.TraceAnnotator(thickness=4)\n",
    "\n",
    "def callback(frame: np.ndarray) -> sv.Detections:\n",
    "    result = model(frame)[0]\n",
    "    detection_model = sv.Detections.from_ultralytics(result)\n",
    "    detections = tracker.update_with_detections(detection_model)\n",
    "\n",
    "    # define labels to show on each bounding box\n",
    "    labels = [\n",
    "        f'#{tracker_id} {confidence:0.2f}'\n",
    "        for tracker_id, confidence \n",
    "        in zip(detections.tracker_id, detections.confidence)\n",
    "    ]\n",
    "\n",
    "    # annotation on frame \n",
    "    annotated_frame = bounding_box_annotator.annotate(\n",
    "        scene = frame.copy(),\n",
    "        detections = detections)\n",
    "\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        annotated_frame, \n",
    "        detections, \n",
    "        labels)\n",
    "    \n",
    "    annotated_frame = trace_annotator.annotate(\n",
    "        annotated_frame,\n",
    "        detections= detections\n",
    "    )\n",
    "\n",
    "    # update in_count and out_count based on objects that cross the line\n",
    "    line_zone.trigger(detections)\n",
    "\n",
    "    return line_zone_annotator.annotate(annotated_frame, line_counter = line_zone)\n",
    "\n",
    "# show the results on one frame\n",
    "test_annotated_frame = callback(annotated_frame)\n",
    "sv.plot_image(test_annotated_frame, (12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the output repository and output file name\n",
    "MAIN_OUTPUT_PATH = EXPORT + f\"/name_of_output_file\"\n",
    "print(MAIN_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate frames generator\n",
    "frames_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# process video\n",
    "with sv.VideoSink(target_path=MAIN_OUTPUT_PATH, \n",
    "                  video_info=video_info) as sink:\n",
    "  heatmap = None\n",
    "  for i, frame in enumerate(frames_generator):\n",
    "    annotated_frame = callback(frame)\n",
    "    \n",
    "    # to visualize tracking and counting frame by frame\n",
    "    #sv.plot_image(annotated_frame)\n",
    "\n",
    "    # Send as frame to video\n",
    "    sink.write_frame(frame=annotated_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
